#%%
# Description des variables :
# Pregancies : Nombre de grossesses pass√©es par la patiente 
# Glucose : Taux de glucose dans le sang (mg/dL)
# BloodPressure : Tension art√©rielle diastolique (mm Hg)
# SkinThickness : √âpaisseur du pli cutan√© (mm)
# Insulin : Niveau d'insuline s√©rique (mu U/mL)
# BMI : Indice de masse corporelle (poids en kg / (taille en m)^2)
# DiabetesPedigreeFunction : Fonction de pedigree du diab√®te (facteur g√©n√©tique)
# Age : √Çge de la patiente (ann√©es)

# Outcome : R√©sultat du test de d√©pistage du diab√®te (1 = diab√©tique, 0 = non diab√©tique)

# Variables d√©mographique et familiale : age, pregnancies 

# Variables biologiques : glucose, bloodpressure, insulin -> indicateurs m√©dicaux directs 

# Variables antropom√©triques : bmi, skinthickness -> indicateurs li√©s au poids et √† la composition corporelle

# Variables g√©n√©tiques : diabetespedigreefunction -> facteur de risque h√©r√©ditaire

#%%

# Importation des biblioth√®ques n√©cessaires :

#######################################################
# Manipulation des donn√©es :
#######################################################

import pandas as pd
import numpy as np
from datetime import datetime
import os
import json
from pathlib import Path
from IPython.display import display # pour afficher les dataframes dans les notebooks

#######################################################
# Visualisation des donn√©es :
#######################################################

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.io as pio

plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
pio.templates.default = "plotly_white"


#######################################################
# Analyse statistique 
#######################################################

from scipy import stats # tests d'hypoth√®ses, corr√©lation, distributions de proba
from scipy.stats import normaltest, shapiro, anderson # tests de normalit√© d'une distribution
from scipy.stats import pearsonr, spearmanr, kendalltau # tests de corr√©lation (pearson -> correlation lin√©aire - donn√©es quantitatives continues, normale
# spearman  -> corr√©lation de rangs - donn√©es non-normales ou ordinales ; kendalltau -> corr√©lation de rangs - petits enchantillons
import statsmodels.api as sm # mod√©lisation statistique (r√©gression, ANOVA, tests, s√©ries temporelles
# -> Sert √† aller plus loin que scikit-learn en analyse statistique)
from statsmodels.stats.outliers_influence import variance_inflation_factor # pour d√©tecter la multicolin√©arit√© entre variables explicatives
# VIF Variance Inflation Factor ~= 1 -> pas de corr√©lation ; 1-5 mod√©r√©e acceptable ; >10 forte corr√©lation


#######################################################
# Machine Learning 
#######################################################

## Pre-processing 


from sklearn.model_selection import (train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, KFold, StratifiedKFold, cross_validate )
# train_test_split : diviser les donn√©es en ensembles d'entra√Ænement et de test
# Kfol : validation crois√©e simple
# stratifiedKFold : validation crois√©e en conservant la proportion des classes
# cross_val_score : √©valuer un mod√®le avec validation crois√©e
# cross_validate : √©valuer plusieurs m√©triques avec validation crois√©e 
# gridSearchCV : recherche exhaustive d'hyperparam√®tres -> teste toutes les combinaisons possibles
# randomizedSearchCV : recherche al√©atoire d'hyperparam√®tres -> teste un nombre fixe

from sklearn.preprocessing import (StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer, LabelEncoder, OneHotEncoder)
# StandardScaler : centrage-r√©duction (moyenne=0, √©cart-type=1) -> Reg lin√©aire, SVM, KNN, PCA
# MinMaxScaler : mise √† l'√©chelle entre 0 et 1 -> r√©seaux de neurones
# RobustScaler : mise √† l'√©chelle robuste aux outliers -> donn√©es avec outliers
# QuantileTransformer : transformation non-lin√©aire pour rendre les donn√©es plus normales ou uniforme -> mod√®les lin√©aires
# LabelEncoder : encoder les labels cat√©goriels en entiers -> variables cibles
# OneHotEncoder : encoder les variables cat√©gorielles en variables binaires -> variables explicatives
from sklearn.impute import SimpleImputer, KNNImputer
# SimpleImputer : imputation des valeurs manquantes avec moyenne, m√©diane, mode ou constante
# KNNImputer : imputation des valeurs manquantes avec les k plus proches voisins
from sklearn.feature_selection import (SelectKBest, f_regression, mutual_info_regression, RFE, RFECV, SelectFromModel)
# SelectKBest : s√©lection des k meilleures caract√©ristiques selon un test statistique, score
# f_regression : test F pour la r√©gression lin√©aire - score pour selectKBest
# mutual_info_regression : information mutuelle pour la r√©gression - score pour selectKBest
# RFE : √©limination r√©cursive des caract√©ristiques - s√©lection des caract√©ristiques en fonction de l'importance
# RFECV : RFE avec validation crois√©e - s√©lection des caract√©ristiques en fonction de l'importance avec validation crois√©e
# SelectFromModel : s√©lection des caract√©ristiques en fonction de l'importance d'un mod√®le (utiliser coef_ ou feature_importances_)


## Models :

from sklearn.linear_model import (LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression, HuberRegressor, RANSACRegressor, TheilSenRegressor)

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor,
                              AdaBoostRegressor,  BaggingRegressor)
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor


## Metrics

from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    mean_absolute_percentage_error, explained_variance_score,
    max_error, median_absolute_error
)


## Advanced ML Libraries 

import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostRegressor, Pool


## Utility Functions 
import time # time() pour mesurer le temps d'ex√©cution, sleep(s) pour pauses de s sec, perf_counter() pour chronom√©trage haute pr√©cision
import gc #lib√©ration automatique de la m√©moire non utilis√©e par Python -> lib√©rer de la ram
from tqdm import tqdm # barre de progression pour les boucles
tqdm.pandas()


## Set Random Seeds for Reproducibility 
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)



#%% 
# ============================================================================
# HELPER FUNCTIONS FOR ANALYSIS
# ============================================================================

def print_divider(title="", width=80, style="="):
    """
    Create a formatted section divider for better readability
    """
    if title:
        side_width = (width - len(title) - 2) // 2
        print(f"\n{style * side_width} {title} {style * side_width}")
    else:
        print(f"\n{style * width}")

def describe_dataframe(df, name="DataFrame"):
    """
    Comprehensive DataFrame description with memory usage and data types
    """
    print_divider(f"{name} Overview") # Nom du data frame
    print(f"Shape: {df.shape[0]} rows √ó {df.shape[1]} columns") # Nombre de ligne et de colonnes
    print(f"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB") # m√©moire utilis√©e (en MO)
    print(f"\nData Types:") 
    print(df.dtypes.value_counts())# types des variables (int, float etc)
    print(f"\nMissing Values: {df.isnull().sum().sum()}") # nombre de valeurs manquantes
    print(f"Duplicate Rows: {df.duplicated().sum()}") # nombre de lignes dupliqu√©es
    
    return df.info() # montre les colonnes, types et valeurs non nulles

def calculate_vif(df, features): 
    """
    Calculate Variance Inflation Factor for multicollinearity detection
    """
    vif_data = pd.DataFrame()
    vif_data["Feature"] = features
    vif_data["VIF"] = [variance_inflation_factor(df[features].values, i) 
                       for i in range(len(features))] # calcul le VIF pour chaque variable (features)
    return vif_data.sort_values('VIF', ascending=False)

def plot_distribution_analysis(df, column, target_column=None):
    """
    Cr√©e une analyse visuelle compl√®te de la distribution d‚Äôune colonne
    """
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=(f'{column} Distribution', 
                       f'{column} Box Plot',
                       f'{column} Q-Q Plot',
                       f'{column} vs Target' if target_column else 'Violin Plot'),
        specs=[[{'type': 'histogram'}, {'type': 'box'}],
               [{'type': 'scatter'}, {'type': 'violin' if not target_column else 'scatter'}]]
    ) # target_column : variable cible pour relation cible vs colonne
    
    # Histogram
    fig.add_trace(
        go.Histogram(x=df[column], name=column, showlegend=False),
        row=1, col=1
    )
    
    # Box plot
    fig.add_trace(
        go.Box(y=df[column], name=column, showlegend=False),
        row=1, col=2
    )
    
    # Q-Q plot
    theoretical_quantiles = stats.probplot(df[column], dist="norm")[0][0]
    sample_quantiles = stats.probplot(df[column], dist="norm")[0][1]
    fig.add_trace(
        go.Scatter(x=theoretical_quantiles, y=sample_quantiles, 
                  mode='markers', name='Q-Q', showlegend=False),
        row=2, col=1
    )
    
    # Target relationship or Violin plot
    if target_column:
        fig.add_trace(
            go.Scatter(x=df[column], y=df[target_column], 
                      mode='markers', name='vs Target', showlegend=False),
            row=2, col=2
        )
    else:
        fig.add_trace(
            go.Violin(y=df[column], name=column, showlegend=False),
            row=2, col=2
        )
    
    fig.update_layout(height=800, showlegend=False,
                     title_text=f"Distribution Analysis: {column}")
    return fig


#%% 

df = pd.read_csv('diabetes.csv')
describe_dataframe(df, name="Diabetes Dataset")

display(df.describe(include='all').round(2))

#%% 
print_divider("DATA QUALITY REPORT", style="=")

# Create comprehensive quality report
quality_report = pd.DataFrame({
    'Column': df.columns,
    'Data Type': df.dtypes.values,
    'Non-Null Count': df.count().values,
    'Null Count': df.isnull().sum().values,
    'Null %': (df.isnull().sum().values / len(df) * 100).round(2),
    'Unique Values': df.nunique().values,
    'Unique %': (df.nunique().values / len(df) * 100).round(2)
})

# Add additional metrics for numeric columns
numeric_cols = df.select_dtypes(include=[np.number]).columns
for col in numeric_cols:
    quality_report.loc[quality_report['Column'] == col, 'Min'] = df[col].min()
    quality_report.loc[quality_report['Column'] == col, 'Max'] = df[col].max()
    quality_report.loc[quality_report['Column'] == col, 'Mean'] = round(df[col].mean(), 2)
    quality_report.loc[quality_report['Column'] == col, 'Std'] = round(df[col].std(), 2)

display(quality_report)

# Check for data anomalies
print_divider("DATA ANOMALY CHECK")

anomalies = []

# Check for negative values in numeric columns
for col in numeric_cols:
    neg_count = (df[col] < 0).sum()
    if neg_count > 0:
        anomalies.append(f"‚ö†Ô∏è {col}: {neg_count} negative values found")
    else:
        print(f"‚úÖ {col}: No negative values")

# Check for duplicates
dup_count = df.duplicated().sum()
if dup_count > 0:
    anomalies.append(f"‚ö†Ô∏è {dup_count} duplicate rows found")
    print(f"‚ö†Ô∏è Found {dup_count} duplicate rows")
    print("Duplicate rows details:")
    display(df[df.duplicated(keep=False)].sort_values(df.columns.tolist()))
else:
    print("‚úÖ No duplicate rows found")

# Check for outliers using IQR method
print_divider("OUTLIER DETECTION (IQR Method)")

for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    outlier_count = len(outliers)
    
    if outlier_count > 0:
        print(f"üìä {col}:")
        print(f"   - Outliers: {outlier_count} ({outlier_count/len(df)*100:.2f}%)")
        print(f"   - Lower Bound: {lower_bound:.2f}")
        print(f"   - Upper Bound: {upper_bound:.2f}")
        print(f"   - Outlier Range: [{outliers[col].min():.2f}, {outliers[col].max():.2f}]")

if anomalies:
    print_divider("ANOMALY SUMMARY")
    for anomaly in anomalies:
        print(anomaly)
else:
    print("\n‚úÖ No major data anomalies detected!")


# %%

print(f"Mean : ${df['Glucose'].mean():.2f}, Median : ${df['Glucose'].median():.2f}, Std : ${df['Glucose'].std():.2f}")
print(f"Mode : ${df['Glucose'].mode()[0]:.2f}")
print(f"Minimum: ${df['Glucose'].min():,.2f}")
print(f"Maximum: ${df['Glucose'].max():,.2f}")
print(f"Range: ${df['Glucose'].max() - df['Glucose'].min():,.2f}")
print(f"Coefficient of Variation: {(df['Glucose'].std()/df['Glucose'].mean())*100:.2f}%")

# Percentiles
print("\nüìà Percentile Distribution:")
percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]
for p in percentiles:
    print(f"  {p}th percentile: ${df['Glucose'].quantile(p/100):,.2f}")

print("\nüìê Distribution Shape:")
print(f"Skewness: {df['Glucose'].skew():.3f}") # mesure la sym√©trie de la distribution par rapport √† sa moyenne.
print(f"Kurtosis: {df['Glucose'].kurtosis():.3f}") # mesure la ‚Äúpointedness‚Äù ou aplatissement de la distribution compar√©e √† une loi normale.

# Interpretation
if df['Glucose'].skew() > 1:
    print("‚Üí Highly right-skewed distribution (long tail to the right)")
elif df['Glucose'].skew() > 0.5:
    print("‚Üí Moderately right-skewed distribution")
elif df['Glucose'].skew() > -0.5:
    print("‚Üí Approximately symmetric distribution")
else:
    print("‚Üí Left-skewed distribution")
# %%
display(plot_distribution_analysis(df, 'Glucose'))
display(plot_distribution_analysis(df, 'BMI'))

display(plot_distribution_analysis(df, 'DiabetesPedigreeFunction', target_column='Outcome'))

# %%
